# OpenAI Configuration (Required for production)
# Default provider is OpenAI - changing this requires explicit consent (see docs/OPENAI_ONLY.md)
VITE_DEFAULT_PROVIDER=openai

# Relay Configuration (Recommended for production)
# When true, routes OpenAI requests through the relay endpoint
# This is the recommended approach for production deployments
VITE_USE_RELAY=true

# OpenAI API Configuration
# Only used when VITE_USE_RELAY=false (local/staging with direct API access)
# VITE_OPENAI_API_KEY=sk-...
# VITE_OPENAI_MODEL=gpt-4o-mini

# API Keys (Disabled by default for security)
# Users should configure API keys in the app settings, not in environment variables
VITE_DEFAULT_KEYS_ENABLED=false

# Relay Endpoint
# Path to the relay endpoint (default: /relay/openai/chat)
# VITE_RELAY_ENDPOINT=/relay/openai/chat

# Gemini Configuration (Optional, experimental, consent-gated)
# Enabling Gemini requires explicit consent from TBR3661 (see docs/OPENAI_ONLY.md)
# VITE_GEMINI_ENABLED=false
# VITE_GEMINI_API_KEY=...
# VITE_ENTITY_PROVIDER_entity1=gemini
# VITE_ENTITY_PROVIDER_entity2=gemini

# Other Provider Configuration (Future, consent-gated)
# Enabling other providers requires explicit consent (see docs/OPENAI_ONLY.md)
# VITE_ANTHROPIC_ENABLED=false
# VITE_ANTHROPIC_API_KEY=...

# Vertex AI Bridge Configuration (Optional, OAuth/Workspace-secured)
# Vertex bridge (Cloud Run/Functions) URL; required when provider = vertex
VERTEX_BRIDGE_URL=https://vertex-bridge-XXXX-uc.a.run.app
# Optional overrides used by the bridge (set in Cloud Run env ideally)
VERTEX_MODEL=gemini-1.5-flash-001
GCP_PROJECT=your-project-id
GCP_LOCATION=us-central1
